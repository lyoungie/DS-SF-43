{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning & Decision Trees Lesson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>Goals</b>\n",
    "\n",
    "- Build from scratch our own machine learning algorithm that we'll use to train on the iris dataset.\n",
    "\n",
    "- Decision Trees algorithm.\n",
    "    - Our very first machine learning algorithm. Learn how it works and how to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our own machine learning classifier with the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in the iris dataset from seaborn\n",
    "iris = sns.load_dataset(\"iris\")\n",
    "\n",
    "#Look at first five rows\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What are the classes of the target variable?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our mission is to create a model that can differentiate between the three classes using the features of sepal length, sepal width, petal length, and petal width."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time for some quick EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group the data by species and derive the average values of each attribute in each category\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this tell us about the differences and similarities of each species?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use a pairplot to visualize the dataset and use the species column to color encode the plots\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again what does this tell us? How can we use this plot to our advantage when we create our machine learning model? What are the vertical and horizontal boundaries between the classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan: \n",
    "- Build a function that uses control flow to make predictions. \n",
    "    - If feature A is less than 3 then return class 1, if not then return class 2.\n",
    "    - If feature B is greater than 4, check to see if feature C  is greater than 1, if so then return class 3, if not then return class 1.\n",
    "    \n",
    "- Use EDA to determine the decision boundaries for this model. If petal_length > 5 then return class ? Or check other conditions.\n",
    "\n",
    "- Make a python function that uses our decision boundaries and apply it to our dataframe. Measure the accuracy of our \"model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7 minute exercise: Work with a partner to determine the boundaries. Don't worry about coding a function, take notes on your boundaries.\n",
    "\n",
    "Be prepared to share your boundary coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x.feature_name allow us to apply function to data frame as opposed to a series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple version of model\n",
    "def iris_model(x):\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply model on to iris data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use model on data by caling the .apply() method on the iris data frame with axis = 1\n",
    "#Assign it to variable called preds\n",
    "\n",
    "preds = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take a look at the predictions\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy score metric: The percentage of correct predictions. Number of correct divided by total number of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a function that tests the accuracy of the predictions\n",
    "\n",
    "def model_tester(labels, predictions):\n",
    "    #Assumes lengths of labels and predictions are the same  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The big moment!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign species column to y \n",
    "y = \n",
    "\n",
    "#Pass the labels and predictions in to the model_tester function\n",
    "#Assign result to accuracy_score variable\n",
    "score =\n",
    "\n",
    "print (\"The model correctly classified {} percent of the data.\".format(score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b> CONGRATULATIONS ON MAKING YOUR FIRST SUCCESSFUL MACHINE LEARNING MODEL!! <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's a closer look to see what we got right and wrong. We are going to compare and contrast the labels and our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use pd.crosstab() to make a cross-tabulated data frame of the labels and predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual labels are the index values and the predicted labels are the columns. Known as a <b>confusion matrix</b>.\n",
    "<br><br>\n",
    "What does this tell us about our model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions so far???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One of the most popular and oldest machine learning models.\n",
    "- Both a classification and regression supervised model.\n",
    "- Intuitive. Very obvious why it's called \"Decision Trees.\" \n",
    "- Foundation is asking a series of questions designed to zero-in on the classification. Sound familiar? (Hint: 20 questions.)\n",
    "- Gateway model to more complex models such as Random Forests and Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agenda: \n",
    "\n",
    "- Interpret a tree diagram\n",
    "- Explain how a decision tree is created\n",
    "- Build a decision tree model in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How a Decision Tree works\n",
    "What makes Decision Trees great is that you can see the process in how it determines a classification.\n",
    "<br><br>\n",
    "Below is a decision tree that models data from the 2008 Democratic primary. It is predicting whether or not a county voted for Hillary Clinton or Barack Obama based on demographics and other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Stuff](https://static01.nyt.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:**\n",
    "\n",
    "- What are the observations? How many observations are there?\n",
    "- What is the response variable?\n",
    "- What are the features?\n",
    "- How are the predictions made?\n",
    "- What is the most predictive feature?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first split is \"County > 20% Black Population\". In this example, when a splitting rule is <b>True</b>, the model follows the right branch and when the splitting rule is <b>False</b>, it follows the left branch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which candidate would the model predict winning this county?\n",
    "15% Black, 82% HS graduation rate, located in Michigan, 60% earned less than $30,000\n",
    "<br><br>\n",
    "Whats the probability for this prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Number of questions indicates the depth of the model, which measures the complexitiy of a model. \n",
    "- Model splits until every observation that satisfies a series of conditions is of the same class or until the model reaches the maximum depth.\n",
    "- Setting a maximum depth is known as \"pruning\" and not setting one or choosing too big of one can lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Why did this model make those decisions? Why is fewer than 20% black the first decision? Why 20% instead of 16 or 25?</b>\n",
    "- At each node, we choose the decision that provides the best split, this is known as maximixing the information gain.\n",
    "- The point where the split is made is determined by which point yields the lowest gini coefficient which measures the quality of a split (0 means perfect split.)\n",
    "- Order of decisions is based on feature importance. The first decision is made on the model's most important feature. Very valuable method for feature selection and engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import model from scikit learn library\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We're going to use sklearn to make some fake data.\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "#Generate fake data that is 500 x 2.\n",
    "data = make_classification(n_samples=500, n_features=2, n_informative=2, n_redundant=0, \n",
    "                    class_sep=.70, random_state = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[0] is the features\n",
    "# data[1] is the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to pandas data frame\n",
    "df = pd.DataFrame(data[0], columns=[\"feature1\", \"feature2\"])\n",
    "#Add target variable to df \n",
    "df[\"target\"] = data[1]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot data with its color-encoded labels. Try to imagine how a decision tree would classify this data. Remember decision trees boundaries are vertical or horizontal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call scatter plot of feature1 vs feature2 with color-encoded target variable\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "plt.figure(figsize=(11, 8))\n",
    "#Color encode target variable\n",
    "colors = df.target.map({0:\"b\", 1:\"r\"})\n",
    "plt.scatter(df.feature1, df.feature2, c = colors, s = 100, alpha=.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start thinking about how a model would classify red vs blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to see what the model says.\n",
    "<br><br>\n",
    "We are going to use the data to <b>train</b> or <b>fit</b> the model. The empty model \"learns\" from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign features to variable X\n",
    "X = \n",
    "#Assign target column to variable y\n",
    "y = \n",
    "\n",
    "#Fit a Decision Tree model with 2 depth on the data.\n",
    "\n",
    "#1. Intialize the model, set max_depth = 2\n",
    "model = \n",
    "#2. Fit/train the model. \n",
    "\n",
    "\n",
    "#The model specifications "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "point = [[0, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call predict on model and pass point vari\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you want the probabilities of each class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call predict_proba()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How strong is the prediction for each class?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how good our model by scoring the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call .score() on model object and pass in the features and labels\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is what's known as the accuracy score or the percentage of prediction we got correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another way to do this\n",
    "\n",
    "#Make predictions on the features\n",
    "preds = model.predict(X)\n",
    "\n",
    "#Sum up all the rows that match and divide by total number of rows\n",
    "sum(preds == y)/float(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using sklearn accuracy score function\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to use a plotting function to visualize the model.\n",
    "\n",
    "What do you think that means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrain model but this time with depth 3\n",
    "model = DecisionTreeClassifier(max_depth=3)\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y):\n",
    "    X_max = X.max(axis=0)\n",
    "    X_min = X.min(axis=0)\n",
    "    xticks = np.linspace(X_min[0], X_max[0], 100)\n",
    "    yticks = np.linspace(X_min[1], X_max[1], 100)\n",
    "    xx, yy = np.meshgrid(xticks, yticks)\n",
    "    ZZ = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = ZZ >= 0.5\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.rcParams[\"figure.figsize\"] = (10,7)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = plt.gca()\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.bwr, alpha=0.2)\n",
    "    ax.scatter(X[:,0], X[:,1], c=y, alpha=0.4, s = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass in model and data into function.\n",
    "#Input a pretrained model, then features, then color-encoded target variables\n",
    "plot_decision_boundary(model, X.values, colors);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! These are the decision boundaries of decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More examples of decision tree boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Four blobs \n",
    "\n",
    "![Example1](https://image.ibb.co/m9EjYG/decisionblobs.png)\n",
    "\n",
    "\n",
    "\n",
    "Credit: [Jake VanderPlas](https://jakevdp.github.io/PythonDataScienceHandbook/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how different decision tree models with varying depth values interpret the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Blobs](https://image.ibb.co/hRE7nb/decisionblobboundaries.png)\n",
    "Credit: Jake VanderPlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to Iris dataset/tree visualization\n",
    "\n",
    "<br><br>\n",
    "We're going to train a decision tree model on the iris dataset and then use visualize the actual decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assign iris features to X and species to y\n",
    "\n",
    "X = iris.drop(\"species\", axis=1)\n",
    "y = iris.species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit decision tree model on iris data with max depth set to 3\n",
    "\n",
    "iris_model = DecisionTreeClassifier(max_depth=3)\n",
    "iris_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Score the model.\n",
    "iris_model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good right? A little bit better than our hand-coded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the decision trees using graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "#Export the decision tree graph viz object. We have to export and the re-import it\n",
    "export_graphviz(iris_model, out_file='iris.dot', \n",
    "                    feature_names=X.columns, \n",
    "                    class_names=y.unique())\n",
    "with open(\"iris.dot\") as f: \n",
    "        dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes and resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees pros and cons:\n",
    "<br><br>\n",
    "**Advantages of decision trees:**\n",
    "- Can be used for regression or classification\n",
    "- Can be displayed graphically\n",
    "- Highly interpretable\n",
    "- Can be specified as a series of rules, and more closely approximate human decision-making than other models\n",
    "- Prediction is fast\n",
    "- Features don't need scaling\n",
    "- Automatically learns feature interactions\n",
    "- Tends to ignore irrelevant features\n",
    "- Non-parametric (will outperform linear models if relationship between features and response is highly non-linear)\n",
    "\n",
    "**Disadvantages of decision trees:**\n",
    "- Performance is (generally) not competitive with the best supervised learning methods (low bias)\n",
    "- Can easily overfit the training data (tuning is required)\n",
    "- Small variations in the data can result in a completely different tree (high variance)\n",
    "- Recursive binary splitting makes \"locally optimal\" decisions that may not result in a globally optimal tree\n",
    "- Doesn't tend to work well if the classes are highly unbalanced\n",
    "- Doesn't tend to work well with very small datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS Gini section\n",
    "<br><br>\n",
    "Common options for the splitting criteria:\n",
    "\n",
    "- **classification error rate:** fraction of training observations in a region that don't belong to the most common class\n",
    "- **Gini index:** measure of total variance across classes in a region\n",
    "\n",
    "#### Example of classification error rate\n",
    "\n",
    "Pretend we are predicting whether someone buys an iPhone or an Android:\n",
    "\n",
    "- At a particular node, there are **25 observations** (phone buyers), of whom **10 bought iPhones and 15 bought Androids**.\n",
    "- Since the majority class is **Android**, that's our prediction for all 25 observations, and thus the classification error rate is **10/25 = 40%**.\n",
    "\n",
    "Our goal in making splits is to **reduce the classification error rate**. Let's try splitting on gender:\n",
    "\n",
    "- **Males:** 2 iPhones and 12 Androids, thus the predicted class is Android\n",
    "- **Females:** 8 iPhones and 3 Androids, thus the predicted class is iPhone\n",
    "- Classification error rate after this split would be **5/25 = 20%**\n",
    "\n",
    "Compare that with a split on age:\n",
    "\n",
    "- **30 or younger:** 4 iPhones and 8 Androids, thus the predicted class is Android\n",
    "- **31 or older:** 6 iPhones and 7 Androids, thus the predicted class is Android\n",
    "- Classification error rate after this split would be **10/25 = 40%**\n",
    "\n",
    "The decision tree algorithm will try **every possible split across all features**, and choose the split that **reduces the error rate the most.**\n",
    "\n",
    "#### Example of Gini index\n",
    "\n",
    "Calculate the Gini index before making a split:\n",
    "\n",
    "$$1 - \\left(\\frac {iPhone} {Total}\\right)^2 - \\left(\\frac {Android} {Total}\\right)^2 = 1 - \\left(\\frac {10} {25}\\right)^2 - \\left(\\frac {15} {25}\\right)^2 = 0.48$$\n",
    "\n",
    "- The **maximum value** of the Gini index is 0.5, and occurs when the classes are perfectly balanced in a node.\n",
    "- The **minimum value** of the Gini index is 0, and occurs when there is only one class represented in a node.\n",
    "- A node with a lower Gini index is said to be more \"pure\".\n",
    "\n",
    "Evaluating the split on **gender** using Gini index:\n",
    "\n",
    "$$\\text{Males: } 1 - \\left(\\frac {2} {14}\\right)^2 - \\left(\\frac {12} {14}\\right)^2 = 0.24$$\n",
    "$$\\text{Females: } 1 - \\left(\\frac {8} {11}\\right)^2 - \\left(\\frac {3} {11}\\right)^2 = 0.40$$\n",
    "$$\\text{Weighted Average: } 0.24 \\left(\\frac {14} {25}\\right) + 0.40 \\left(\\frac {11} {25}\\right) = 0.31$$\n",
    "\n",
    "Evaluating the split on **age** using Gini index:\n",
    "\n",
    "$$\\text{30 or younger: } 1 - \\left(\\frac {4} {12}\\right)^2 - \\left(\\frac {8} {12}\\right)^2 = 0.44$$\n",
    "$$\\text{31 or older: } 1 - \\left(\\frac {6} {13}\\right)^2 - \\left(\\frac {7} {13}\\right)^2 = 0.50$$\n",
    "$$\\text{Weighted Average: } 0.44 \\left(\\frac {12} {25}\\right) + 0.50 \\left(\\frac {13} {25}\\right) = 0.47$$\n",
    "\n",
    "Again, the decision tree algorithm will try **every possible split**, and will choose the split that **reduces the Gini index (and thus increases the \"node purity\") the most.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree resources\n",
    "- http://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/\n",
    "- http://dataaspirant.com/2017/02/01/decision-tree-algorithm-python-with-scikit-learn/\n",
    "- https://www.youtube.com/watch?v=eKD5gxPPeY0&t=7s\n",
    "- https://github.com/josiahdavis/DecisionTrees/blob/master/DecisionTrees.ipynb\n",
    "- https://www.talend.com/blog/2016/09/29/machine-learning-made-easy-with-talend-decision-trees/\n",
    "- https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine learning resources\n",
    "\n",
    "- Intro to ML in sklearn http://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "- https://www.digitalocean.com/community/tutorials/an-introduction-to-machine-learning\n",
    "- https://www.toptal.com/machine-learning/machine-learning-theory-an-introductory-primer\n",
    "- https://www.youtube.com/watch?v=IpGxLWOIZy4\n",
    "- https://www.analyticsvidhya.com/blog/2015/06/machine-learning-basics/\n",
    "- [Visual introduction to machine learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
    "- Great series -> https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
